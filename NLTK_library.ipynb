{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4",
      "authorship_tag": "ABX9TyPSVnV1+WNwGrVRzLZ3jx0B"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# NLTK library(Natural Language Toolkit)\n",
        "\n",
        "NLTK (Natural Language Toolkit) is a widely used open-source Python library for Natural Language Processing (NLP). It provides a wide range of text-processing libraries, corpora, and lexical resources for research and education. NLTK is designed to make working with human language data easy and efficient, offering tools for tasks such as tokenization, stemming, lemmatization, part-of-speech tagging, parsing, and semantic reasoning.\n",
        "\n",
        "---\n",
        "\n",
        "## Key Features\n",
        "1. **Tokenization** – Splitting text into words or sentences.\n",
        "2. **Text Normalization** – Lowercasing, removing punctuation, and standardizing words.\n",
        "3. **Stemming and Lemmatization** – Reducing words to their root or base form.\n",
        "4. **Part-of-Speech (POS) Tagging** – Assigning grammatical tags to words.\n",
        "5. **Named Entity Recognition (NER)** – Identifying entities like names, locations, and organizations.\n",
        "6. **Parsing and Syntax Trees** – Analyzing grammatical structure.\n",
        "7. **Corpora Access** – Built-in datasets like Brown Corpus, Gutenberg Corpus, and WordNet.\n",
        "8. **Text Classification** – Building and evaluating simple classifiers.\n",
        "9. **Integration with Other Libraries** – Works with scikit-learn, pandas, and more.\n",
        "\n",
        "---\n",
        "\n",
        "## Common NLP Tasks with NLTK\n",
        "- **Tokenization:** Breaking down text into words or sentences.\n",
        "- **Stopword Removal:** Filtering out commonly used words that do not add meaning.\n",
        "- **Stemming:** Trimming words to their crude root (e.g., “running” → “run”).\n",
        "- **Lemmatization:** Using vocabulary to get proper base forms (e.g., “better” → “good”).\n",
        "- **POS Tagging:** Identifying if a word is a noun, verb, adjective, etc.\n",
        "- **NER:** Detecting proper nouns and classifying them into categories.\n",
        "- **Parsing:** Understanding sentence grammar.\n",
        "- **Text Classification:** Categorizing text into predefined classes.\n",
        "\n",
        "---\n",
        "\n",
        "## Advantages of NLTK\n",
        "- Easy to use and well-documented.\n",
        "- Large collection of corpora and lexical resources.\n",
        "- Rich set of NLP algorithms for academic and educational purposes.\n",
        "- Flexible for experimentation and prototyping.\n",
        "\n",
        "---\n",
        "\n",
        "## Limitations of NLTK\n",
        "- Slower than modern libraries like spaCy for large-scale processing.\n",
        "- Models are not always state-of-the-art.\n",
        "- Requires downloading datasets before use.\n",
        "\n"
      ],
      "metadata": {
        "id": "hl9OZZE4HvDP"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "UnLJtVLEHmHX",
        "outputId": "2f4c3439-1152-4567-e6a0-1b3740f680f4"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: nltk in /usr/local/lib/python3.11/dist-packages (3.9.1)\n",
            "Requirement already satisfied: click in /usr/local/lib/python3.11/dist-packages (from nltk) (8.2.1)\n",
            "Requirement already satisfied: joblib in /usr/local/lib/python3.11/dist-packages (from nltk) (1.5.1)\n",
            "Requirement already satisfied: regex>=2021.8.3 in /usr/local/lib/python3.11/dist-packages (from nltk) (2024.11.6)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.11/dist-packages (from nltk) (4.67.1)\n"
          ]
        }
      ],
      "source": [
        "pip install nltk"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import nltk\n",
        "nltk.download('punkt')\n",
        "nltk.download('stopwords')\n",
        "nltk.download('wordnet')\n",
        "nltk.download('averaged_perceptron_tagger')\n",
        "nltk.download('maxent_ne_chunker')\n",
        "nltk.download('words')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "UCxtMx9QIvvr",
        "outputId": "52435478-dcc9-43b0-935c-95c003f57753"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Unzipping tokenizers/punkt.zip.\n",
            "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]   Unzipping corpora/stopwords.zip.\n",
            "[nltk_data] Downloading package wordnet to /root/nltk_data...\n",
            "[nltk_data] Downloading package averaged_perceptron_tagger to\n",
            "[nltk_data]     /root/nltk_data...\n",
            "[nltk_data]   Unzipping taggers/averaged_perceptron_tagger.zip.\n",
            "[nltk_data] Downloading package maxent_ne_chunker to\n",
            "[nltk_data]     /root/nltk_data...\n",
            "[nltk_data]   Unzipping chunkers/maxent_ne_chunker.zip.\n",
            "[nltk_data] Downloading package words to /root/nltk_data...\n",
            "[nltk_data]   Unzipping corpora/words.zip.\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "metadata": {},
          "execution_count": 2
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "nltk.download('gutenberg')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "uOYB9XnNJiKg",
        "outputId": "cc69191c-26cd-4e9a-ee2f-11cb393dbbe6"
      },
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package gutenberg to /root/nltk_data...\n",
            "[nltk_data]   Unzipping corpora/gutenberg.zip.\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "metadata": {},
          "execution_count": 4
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from nltk.corpus import gutenberg\n",
        "print(gutenberg.fileids())\n",
        "print(gutenberg.raw('austen-emma.txt')[:500])"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "RnWiB0iGJPq5",
        "outputId": "3fe42bfb-a2a8-44e0-d275-88bc4c69e28d"
      },
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "['austen-emma.txt', 'austen-persuasion.txt', 'austen-sense.txt', 'bible-kjv.txt', 'blake-poems.txt', 'bryant-stories.txt', 'burgess-busterbrown.txt', 'carroll-alice.txt', 'chesterton-ball.txt', 'chesterton-brown.txt', 'chesterton-thursday.txt', 'edgeworth-parents.txt', 'melville-moby_dick.txt', 'milton-paradise.txt', 'shakespeare-caesar.txt', 'shakespeare-hamlet.txt', 'shakespeare-macbeth.txt', 'whitman-leaves.txt']\n",
            "[Emma by Jane Austen 1816]\n",
            "\n",
            "VOLUME I\n",
            "\n",
            "CHAPTER I\n",
            "\n",
            "\n",
            "Emma Woodhouse, handsome, clever, and rich, with a comfortable home\n",
            "and happy disposition, seemed to unite some of the best blessings\n",
            "of existence; and had lived nearly twenty-one years in the world\n",
            "with very little to distress or vex her.\n",
            "\n",
            "She was the youngest of the two daughters of a most affectionate,\n",
            "indulgent father; and had, in consequence of her sister's marriage,\n",
            "been mistress of his house from a very early period.  Her mother\n",
            "had died t\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Tokenization\n",
        "\n",
        "Goal: Break text into sentences or words."
      ],
      "metadata": {
        "id": "cDGxlcT_Jr7U"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "nltk.download('punkt')\n",
        "nltk.download('punkt_tab')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "j_09tVZhJ8Qr",
        "outputId": "7de61ff5-ce75-4707-9537-fc404d676c74"
      },
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Package punkt is already up-to-date!\n",
            "[nltk_data] Downloading package punkt_tab to /root/nltk_data...\n",
            "[nltk_data]   Unzipping tokenizers/punkt_tab.zip.\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "metadata": {},
          "execution_count": 7
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Word Tokenization:\n",
        "\n",
        "from nltk.tokenize import word_tokenize\n",
        "text = \"NLTK is great for text processing!\"\n",
        "words = word_tokenize(text)\n",
        "print(words)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "hmPRWtF3JSKE",
        "outputId": "c5b2c6f0-bdf7-403c-c6b9-9c76aae03e64"
      },
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "['NLTK', 'is', 'great', 'for', 'text', 'processing', '!']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Sentence Tokenization:\n",
        "\n",
        "from nltk.tokenize import sent_tokenize\n",
        "sentences = sent_tokenize(text)\n",
        "print(sentences)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "zIinZhbvJwjJ",
        "outputId": "97649ed0-b83d-40e1-e5f9-2cd8eb14edee"
      },
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "['NLTK is great for text processing!']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Stopwords Removal\n",
        "\n",
        "Goal: Remove common, meaningless words."
      ],
      "metadata": {
        "id": "rFKo-B1iKFKN"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from nltk.corpus import stopwords\n",
        "stop_words = set(stopwords.words('english'))\n",
        "filtered = [w for w in words if w.lower() not in stop_words]\n",
        "print(filtered)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "i9ASb6XbKC0J",
        "outputId": "960cc25e-8c09-45f9-f2a3-c380ef63a566"
      },
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "['NLTK', 'great', 'text', 'processing', '!']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Stemming\n",
        "\n",
        "Goal: Reduce words to their root form."
      ],
      "metadata": {
        "id": "muFovXTiKLG9"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from nltk.stem import PorterStemmer\n",
        "ps = PorterStemmer()\n",
        "print(ps.stem(\"running\"))  # run"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "sLsesDjZKHeh",
        "outputId": "f24ec2a5-1d7b-4370-da5c-bc8af470c8a3"
      },
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "run\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Lemmatization\n",
        "\n",
        "Goal: Reduce words to their base dictionary form."
      ],
      "metadata": {
        "id": "JsBIgBGHKQld"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from nltk.stem import WordNetLemmatizer\n",
        "nltk.download('omw-1.4')\n",
        "\n",
        "lemmatizer = WordNetLemmatizer()\n",
        "print(lemmatizer.lemmatize(\"running\", pos=\"v\"))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "jdNhluDWKNJR",
        "outputId": "a9d09838-cdd3-4d52-8418-e51e7dda4987"
      },
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package omw-1.4 to /root/nltk_data...\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "run\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print(lemmatizer.lemmatize(\"fighting\", pos=\"v\"))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "P_am971HKSsZ",
        "outputId": "381be6a7-a9ca-46d4-ac46-41d1f2032329"
      },
      "execution_count": 13,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "fight\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Part-of-Speech (POS) Tagging\n",
        "\n",
        "Goal: Identify parts of speech for each word."
      ],
      "metadata": {
        "id": "aZZiBgoPKauV"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "nltk.download('averaged_perceptron_tagger_eng')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "z3OJtuMuKrs6",
        "outputId": "437b3f0e-356b-4c5b-d8d8-7fd41141613c"
      },
      "execution_count": 15,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package averaged_perceptron_tagger_eng to\n",
            "[nltk_data]     /root/nltk_data...\n",
            "[nltk_data]   Unzipping taggers/averaged_perceptron_tagger_eng.zip.\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "metadata": {},
          "execution_count": 15
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from nltk import pos_tag\n",
        "pos = pos_tag(words)\n",
        "print(pos)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "uda4UY-qKYhR",
        "outputId": "055f0f8b-8b81-4f5b-91ef-8293831da4eb"
      },
      "execution_count": 16,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[('NLTK', 'NNP'), ('is', 'VBZ'), ('great', 'JJ'), ('for', 'IN'), ('text', 'JJ'), ('processing', 'NN'), ('!', '.')]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Named Entity Recognition (NER)\n",
        "\n",
        "Goal: Detect names, places, organizations."
      ],
      "metadata": {
        "id": "KjxiwvfyKxr0"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "nltk.download('maxent_ne_chunker_tab')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "it1eBCiWK_yy",
        "outputId": "cf782584-df02-4490-c8b3-a14bd47ab06e"
      },
      "execution_count": 18,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package maxent_ne_chunker_tab to\n",
            "[nltk_data]     /root/nltk_data...\n",
            "[nltk_data]   Unzipping chunkers/maxent_ne_chunker_tab.zip.\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "metadata": {},
          "execution_count": 18
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from nltk import ne_chunk\n",
        "nltk.download('maxent_ne_chunker')\n",
        "nltk.download('words')\n",
        "\n",
        "entities = ne_chunk(pos_tag(word_tokenize(\"Apple is looking at buying U.K. startup for $1 billion\")))\n",
        "print(entities)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "DMEtLaXEKeTp",
        "outputId": "90e4d247-87f1-41d4-fa74-3db65e76c169"
      },
      "execution_count": 19,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package maxent_ne_chunker to\n",
            "[nltk_data]     /root/nltk_data...\n",
            "[nltk_data]   Package maxent_ne_chunker is already up-to-date!\n",
            "[nltk_data] Downloading package words to /root/nltk_data...\n",
            "[nltk_data]   Package words is already up-to-date!\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "(S\n",
            "  (GPE Apple/NNP)\n",
            "  is/VBZ\n",
            "  looking/VBG\n",
            "  at/IN\n",
            "  buying/VBG\n",
            "  U.K./NNP\n",
            "  startup/NN\n",
            "  for/IN\n",
            "  $/$\n",
            "  1/CD\n",
            "  billion/CD)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Chunking & Chinking\n",
        "\n",
        "Goal: Extract meaningful phrases from text."
      ],
      "metadata": {
        "id": "avfWpZ9wLHJ_"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from nltk.chunk import RegexpParser\n",
        "pattern = \"NP: {<DT>?<JJ>*<NN>}\"\n",
        "parser = RegexpParser(pattern)\n",
        "result = parser.parse(pos)\n",
        "print(result)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "YqS937QPKz_R",
        "outputId": "0be70e5b-36c4-4ee4-bb00-ab9837af3fc7"
      },
      "execution_count": 20,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "(S NLTK/NNP is/VBZ great/JJ for/IN (NP text/JJ processing/NN) !/.)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# N-grams\n",
        "\n",
        "Goal: Extract sequences of N words."
      ],
      "metadata": {
        "id": "mYEp3jWNLqdJ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from nltk.util import ngrams\n",
        "bigrams = list(ngrams(words, 2))\n",
        "print(bigrams)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "5FIZlk7kLJY5",
        "outputId": "fca89418-6eb3-465a-ac55-5a64ba0b52a9"
      },
      "execution_count": 21,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[('NLTK', 'is'), ('is', 'great'), ('great', 'for'), ('for', 'text'), ('text', 'processing'), ('processing', '!')]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "0o8PIKlVL1Cx"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}